{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import calendar\n",
    "from sklearn import cluster\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Exploration\n",
    "Explore and graph trends and clusters in the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "string_data_cols = [5,6,7,8,9,10,11,12,14,15,16,17,18,19,20,21,22]\n",
    "num_data_cols = [0,1,2,3,4,13]\n",
    "goal = [23]\n",
    "\n",
    "data_frame = pd.read_csv('trainset.csv')\n",
    "df =  pd.read_csv('trainset.csv')\n",
    "data = data_frame.to_numpy()\n",
    "#print(data_frame)\n",
    "for i in range(len(data)):\n",
    "    if data[i, 23] == \"Non-Fraud\":\n",
    "        data[i, 23] = 0.0\n",
    "    else:\n",
    "        data[i, 23] = 1.0\n",
    "data_frame = pd.DataFrame(data)\n",
    "\n",
    "data_numeric = data[:, 0:5]\n",
    "data_goal = data[:, 23]\n",
    "\n",
    "df['GOAL'] = list(data_goal)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Numerical Data Exploration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explore the relationship between the goal and the numerical datatypes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "data_frame_numeric = df[['TRAN_AMT', 'ACCT_PRE_TRAN_AVAIL_BAL', 'CUST_AGE', 'OPEN_ACCT_CT', 'WF_dvc_age', 'GOAL']].copy()\n",
    "plt.matshow(data_frame_numeric.corr(), cmap = 'Blues')\n",
    "plt.show()\n",
    "print(data_frame_numeric.corr())"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                         TRAN_AMT  ACCT_PRE_TRAN_AVAIL_BAL  CUST_AGE  \\\n",
      "TRAN_AMT                 1.000000                 0.095355  0.023688   \n",
      "ACCT_PRE_TRAN_AVAIL_BAL  0.095355                 1.000000 -0.030382   \n",
      "CUST_AGE                 0.023688                -0.030382  1.000000   \n",
      "OPEN_ACCT_CT             0.059517                 0.087180 -0.013212   \n",
      "WF_dvc_age               0.008768                 0.083629 -0.013567   \n",
      "GOAL                     0.336169                -0.011808  0.044426   \n",
      "\n",
      "                         OPEN_ACCT_CT  WF_dvc_age      GOAL  \n",
      "TRAN_AMT                     0.059517    0.008768  0.336169  \n",
      "ACCT_PRE_TRAN_AVAIL_BAL      0.087180    0.083629 -0.011808  \n",
      "CUST_AGE                    -0.013212   -0.013567  0.044426  \n",
      "OPEN_ACCT_CT                 1.000000    0.068204 -0.051024  \n",
      "WF_dvc_age                   0.068204    1.000000 -0.116179  \n",
      "GOAL                        -0.051024   -0.116179  1.000000  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Examining the correlation matrix between fraud and non fraud and the other numerical inputs there is a strong correlation between transaction amount and whether or not the transaction was fraud. None of the other fields had particularly strong correlations. The Next strongest was a -.1 between the WF_dvc_age and Fraud"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### String Data Exploration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a correlation matrix of the strings that I initially believe will be most impactful"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "string_data_cols = [6,7,8,9,10,11,12,14,19,20,21]\n",
    "time_stamp_cols = [5,15,16,17,18, 22]\n",
    "print(df.columns)\n",
    "#These are the strings that I think will wind up being important\n",
    "df_strings_pre_conceived = df[['CARR_NAME','RGN_NAME', 'STATE_PRVNC_TXT', 'DVC_TYPE_TXT', 'AUTHC_PRIM_TYPE_CD', 'AUTHC_SCNDRY_STAT_TXT', 'CUST_STATE', 'FRAUD_NONFRAUD']]\n",
    "for x in df_strings_pre_conceived.columns:\n",
    "    df_strings_pre_conceived[x] = df[x].astype('category').cat.codes\n",
    "\n",
    "plt.matshow(df_strings_pre_conceived.corr(), cmap = 'Blues')\n",
    "plt.show()\n",
    "print(df_strings_pre_conceived.corr())\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['TRAN_AMT', 'ACCT_PRE_TRAN_AVAIL_BAL', 'CUST_AGE', 'OPEN_ACCT_CT',\n",
      "       'WF_dvc_age', 'PWD_UPDT_TS', 'CARR_NAME', 'RGN_NAME', 'STATE_PRVNC_TXT',\n",
      "       'ALERT_TRGR_CD', 'DVC_TYPE_TXT', 'AUTHC_PRIM_TYPE_CD',\n",
      "       'AUTHC_SCNDRY_STAT_TXT', 'CUST_ZIP', 'CUST_STATE', 'PH_NUM_UPDT_TS',\n",
      "       'CUST_SINCE_DT', 'TRAN_TS', 'TRAN_DT', 'ACTN_CD', 'ACTN_INTNL_TXT',\n",
      "       'TRAN_TYPE_CD', 'ACTVY_DT', 'FRAUD_NONFRAUD', 'GOAL'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/var/folders/hl/sjhkfxxd2hjb_h0297r3_74w0000gn/T/ipykernel_54137/2827173379.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_strings_pre_conceived[x] = df[x].astype('category').cat.codes\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                       CARR_NAME  RGN_NAME  STATE_PRVNC_TXT  DVC_TYPE_TXT  \\\n",
      "CARR_NAME               1.000000  0.316613         0.308338      0.048548   \n",
      "RGN_NAME                0.316613  1.000000         0.132644      0.153718   \n",
      "STATE_PRVNC_TXT         0.308338  0.132644         1.000000      0.022787   \n",
      "DVC_TYPE_TXT            0.048548  0.153718         0.022787      1.000000   \n",
      "AUTHC_PRIM_TYPE_CD     -0.062331 -0.069421        -0.076637     -0.008129   \n",
      "AUTHC_SCNDRY_STAT_TXT   0.032824 -0.023836         0.061863     -0.031897   \n",
      "CUST_STATE             -0.000691 -0.033062         0.002353     -0.005450   \n",
      "FRAUD_NONFRAUD          0.125268  0.519975         0.040737      0.283230   \n",
      "\n",
      "                       AUTHC_PRIM_TYPE_CD  AUTHC_SCNDRY_STAT_TXT  CUST_STATE  \\\n",
      "CARR_NAME                       -0.062331               0.032824   -0.000691   \n",
      "RGN_NAME                        -0.069421              -0.023836   -0.033062   \n",
      "STATE_PRVNC_TXT                 -0.076637               0.061863    0.002353   \n",
      "DVC_TYPE_TXT                    -0.008129              -0.031897   -0.005450   \n",
      "AUTHC_PRIM_TYPE_CD               1.000000              -0.049460   -0.013448   \n",
      "AUTHC_SCNDRY_STAT_TXT           -0.049460               1.000000    0.010255   \n",
      "CUST_STATE                      -0.013448               0.010255    1.000000   \n",
      "FRAUD_NONFRAUD                  -0.093419              -0.080805   -0.053100   \n",
      "\n",
      "                       FRAUD_NONFRAUD  \n",
      "CARR_NAME                    0.125268  \n",
      "RGN_NAME                     0.519975  \n",
      "STATE_PRVNC_TXT              0.040737  \n",
      "DVC_TYPE_TXT                 0.283230  \n",
      "AUTHC_PRIM_TYPE_CD          -0.093419  \n",
      "AUTHC_SCNDRY_STAT_TXT       -0.080805  \n",
      "CUST_STATE                  -0.053100  \n",
      "FRAUD_NONFRAUD               1.000000  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It appears that Carrier Name, region name and Device type have strong correlations with whether or not the transaction was considered fraud. These appear to be the strongest indicators\n",
    "\n",
    "This next cell is to see if a true false about whether or not transaction state matching cust state will make a difference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df_state_match = df[['STATE_PRVNC_TXT',  'CUST_STATE', 'FRAUD_NONFRAUD']]\n",
    "states = {\"AL\":\"Alabama\",\"AK\":\"Alaska\",\"AZ\":\"Arizona\",\"AR\":\"Arkansas\",\"CA\":\"California\",\"CO\":\"Colorado\",\"CT\":\"Connecticut\",\"DE\":\"Delaware\",\"FL\":\"Florida\",\"GA\":\"Georgia\",\"HI\":\"Hawaii\",\"ID\":\"Idaho\",\"IL\":\"Illinois\",\"IN\":\"Indiana\",\"IA\":\"Iowa\",\"KS\":\"Kansas\",\"KY\":\"Kentucky\",\"LA\":\"Louisiana\",\"ME\":\"Maine\",\"MD\":\"Maryland\",\"MA\":\"Massachusetts\",\"MI\":\"Michigan\",\"MN\":\"Minnesota\",\"MS\":\"Mississippi\",\"MO\":\"Missouri\",\"MT\":\"Montana\",\"NE\":\"Nebraska\",\"NV\":\"Nevada\",\"NH\":\"New Hampshire\",\"NJ\":\"New Jersey\",\"NM\":\"New Mexico\",\"NY\":\"New York\",\"NC\":\"North Carolina\",\"ND\":\"North Dakota\",\"OH\":\"Ohio\",\"OK\":\"Oklahoma\",\"OR\":\"Oregon\",\"PA\":\"Pennsylvania\",\"RI\":\"Rhode Island\",\"SC\":\"South Carolina\",\"SD\":\"South Dakota\",\"TN\":\"Tennessee\",\"TX\":\"Texas\",\"UT\":\"Utah\",\"VT\":\"Vermont\",\"VA\":\"Virginia\",\"WA\":\"Washington\",\"WV\":\"West Virginia\",\"WI\":\"Wisconsin\",\"WY\":\"Wyoming\"}\n",
    "#df_state_match['match'] = (df['STATE_PRVNC_TXT'] == states[df['CUST_STATE']])\n",
    "state_match = df_state_match.to_numpy()\n",
    "print(state_match)\n",
    "matches = []\n",
    "for row in state_match:\n",
    "    try:\n",
    "        cust_state = row[1]\n",
    "        trans_loc = row[0].lower()\n",
    "        cust_state = states[cust_state.upper()].lower()\n",
    "        matches.append(int(trans_loc == cust_state))\n",
    "    except:\n",
    "        matches.append(False)\n",
    "\n",
    "df_state_match['matches'] = matches\n",
    "\n",
    "df_state_match['matches'] = df_state_match['matches'].astype('category').cat.codes\n",
    "df_state_match['FRAUD_NONFRAUD'] = df_state_match['FRAUD_NONFRAUD'].astype('category').cat.codes\n",
    "#print(type(df_state_match['FRAUD_NONFRAUD']))\n",
    "print(df_state_match[['matches', 'FRAUD_NONFRAUD']].corr())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['nevada' 'NV' 'Non-Fraud']\n",
      " ['california' 'CA' 'Non-Fraud']\n",
      " ['utah' 'MD' 'Fraud']\n",
      " ...\n",
      " ['california' 'FL' 'Fraud']\n",
      " [nan 'CA' 'Fraud']\n",
      " ['texas' 'NJ' 'Non-Fraud']]\n",
      "                 matches  FRAUD_NONFRAUD\n",
      "matches         1.000000        0.206406\n",
      "FRAUD_NONFRAUD  0.206406        1.000000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/var/folders/hl/sjhkfxxd2hjb_h0297r3_74w0000gn/T/ipykernel_54137/1080431116.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_state_match['matches'] = matches\n",
      "/var/folders/hl/sjhkfxxd2hjb_h0297r3_74w0000gn/T/ipykernel_54137/1080431116.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_state_match['matches'] = df_state_match['matches'].astype('category').cat.codes\n",
      "/var/folders/hl/sjhkfxxd2hjb_h0297r3_74w0000gn/T/ipykernel_54137/1080431116.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_state_match['FRAUD_NONFRAUD'] = df_state_match['FRAUD_NONFRAUD'].astype('category').cat.codes\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There's a strong correlation between matching the state and customer state"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Date Time Exploration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explore the Dates and time stamps : create diffs in terms of the number of days between pswd changes and transaction date etc"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "time_stamp_cols = [5,15,16,17,18, 22]\n",
    "#These are the strings that I think will wind up being important\n",
    "df_date_times = df[['PWD_UPDT_TS','TRAN_TS', 'PH_NUM_UPDT_TS', 'CUST_SINCE_DT']].copy()\n",
    "for x in df_date_times.columns:\n",
    "    \n",
    "    df_date_times[x] = pd.to_datetime(df_date_times[x], errors='coerce')\n",
    "\n",
    "df_diffs = pd.DataFrame()\n",
    "for x in df_date_times.columns:\n",
    "    if x != \"TRAN_TS\":\n",
    "        df_diffs[x] = df_date_times['TRAN_TS'] - df_date_times[x]\n",
    "        df_diffs[x] = df_diffs[x].astype('timedelta64[D]')\n",
    "        df_diffs[x] = df_diffs[x].fillna(value = -1)\n",
    "df_diffs['GOAL'] = df['GOAL']\n",
    "#print(df_diffs)\n",
    "\n",
    "plt.matshow(df_diffs.corr(), cmap = 'Blues')\n",
    "plt.show()\n",
    "print(df_diffs.corr())"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAECCAYAAADXWsr9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAIx0lEQVR4nO3dT4ic9R3H8c8nm00TmoAJCRiSUIWIKB4UlrQ0tIeAkHqJ9FDMwZOQk6DQi60nD+rNWy8BQ1vwD0KCilgkh4AGNWYNUUyiJRXFBCG7JKKREol+e9gppHXpTJrnN8/Oft4vWNgZl5nPo3nn2ZldfFxVArC8reh7AID2CB0IQOhAAEIHAhA6EIDQgQATHbrt3bY/sX3W9mN97+mS7QO2L9j+qO8tLdjeZvuI7dO2T9l+pO9NXbG92vZ7tj8YHNsTvW+a1J+j256S9HdJ90o6J+m4pL1VdbrXYR2x/WtJlyX9taru6ntP12xvlrS5qk7YXifpfUn3L4f/frYt6adVddn2tKSjkh6pqnf72jTJZ/Qdks5W1adV9Z2kFyXt6XlTZ6rqTUkX+97RSlV9WVUnBp9/I+mMpC39rupGLbg8uDk9+Oj1jDrJoW+R9MU1t89pmfxBSWP7Fkn3SDrW85TO2J6yfVLSBUmHq6rXY5vk0LEM2F4r6aCkR6vq6773dKWqvq+quyVtlbTDdq8vvyY59POStl1ze+vgPkyIwevXg5Keq6pDfe9poaq+knRE0u4+d0xy6Mcl3Wb7VturJD0g6dWeN2FEgzesnpV0pqqe6XtPl2xvsn3T4PM1WnjD+OM+N01s6FV1VdLDkt7Qwhs5L1XVqX5Xdcf2C5LekXS77XO2H+p7U8d2SnpQ0i7bJwcf9/U9qiObJR2x/aEWTkiHq+q1PgdN7I/XAIxuYs/oAEZH6EAAQgcCEDoQgNCBABMfuu19fW9oieObbEvl+CY+dElL4l9kQxzfZFsSx7ccQgcwRJNfmPHKNeVV6zp/3MXU1X/KK9eM5bn+7Y7tW8f2XJcuzmv9ho1je75x6+P4Vk+P7/w2Nz+nTRs3je35Pv/8M83Pz/u/71/Z4sm8ap1+cvvvWjz0kvD8K0/3PaGpqRU/+nOyrGy/eW3fE5rZ+fOZRe/nW3cgAKEDAQgdCEDoQABCBwIQOhCA0IEAhA4EIHQgAKEDAQgdCEDoQABCBwIQOhCA0IEAhA4EIHQgAKEDAQgdCEDoQABCBwIQOhBgpNBt77b9ie2zth9rPQpAt4aGbntK0p8k/UbSnZL22r6z9TAA3RnljL5D0tmq+rSqvpP0oqQ9bWcB6NIooW+R9MU1t88N7gMwITq7JNPg8rALV46cXr6XvAEm0Shn9POStl1ze+vgvv9QVfuraqaqZsZ90UMA/9sooR+XdJvtW22vkvSApFfbzgLQpaHfulfVVdsPS3pD0pSkA1V1qvkyAJ0Z6TV6Vb0u6fXGWwA0wm/GAQEIHQhA6EAAQgcCEDoQgNCBAIQOBCB0IAChAwEIHQhA6EAAQgcCEDoQgNCBAIQOBCB0IAChAwEIHQhA6EAAQgcCEDoQgNCBAJ1dkulad2zfqudfebrFQy8Jv9jzh74nNPX2y0/1PaGpz+a+7XtCM1eu/rDo/ZzRgQCEDgQgdCAAoQMBCB0IQOhAAEIHAhA6EIDQgQCEDgQgdCAAoQMBCB0IQOhAAEIHAhA6EIDQgQCEDgQgdCAAoQMBCB0IQOhAAEIHAhA6EGBo6LYP2L5g+6NxDALQvVHO6H+WtLvxDgANDQ29qt6UdHEMWwA0wmt0IEBnodveZ3vW9uyli/NdPSyADnQWelXtr6qZqppZv2FjVw8LoAN86w4EGOXHay9IekfS7bbP2X6o/SwAXVo57Auqau84hgBoh2/dgQCEDgQgdCAAoQMBCB0IQOhAAEIHAhA6EIDQgQCEDgQgdCAAoQMBCB0IQOhAAEIHAhA6EIDQgQCEDgQgdCAAoQMBCB0IQOhAgKH/u+f/19QKt3ro3r398lN9T2jql/f/se8JTb116Mm+JzTzww+16P2c0YEAhA4EIHQgAKEDAQgdCEDoQABCBwIQOhCA0IEAhA4EIHQgAKEDAQgdCEDoQABCBwIQOhCA0IEAhA4EIHQgAKEDAQgdCEDoQABCBwIQOhBgaOi2t9k+Yvu07VO2HxnHMADdGeVKLVcl/b6qTtheJ+l924er6nTjbQA6MvSMXlVfVtWJweffSDojaUvrYQC6c12v0W3fIukeScearAHQxMih214r6aCkR6vq60X++T7bs7ZnL12c73IjgBs0Uui2p7UQ+XNVdWixr6mq/VU1U1Uz6zds7HIjgBs0yrvulvSspDNV9Uz7SQC6NsoZfaekByXtsn1y8HFf410AOjT0x2tVdVSSx7AFQCP8ZhwQgNCBAIQOBCB0IAChAwEIHQhA6EAAQgcCEDoQgNCBAIQOBCB0IAChAwEIHQhA6EAAQgcCEDoQgNCBAIQOBCB0IAChAwEIHQgwytVUr9vq6RXafvPaFg+9JHw2923fE5p669CTfU9o6le/fbzvCc1c+cf5Re/njA4EIHQgAKEDAQgdCEDoQABCBwIQOhCA0IEAhA4EIHQgAKEDAQgdCEDoQABCBwIQOhCA0IEAhA4EIHQgAKEDAQgdCEDoQABCBwIQOhCA0IEAQ0O3vdr2e7Y/sH3K9hPjGAagO6NcqeWKpF1Vddn2tKSjtv9WVe823gagI0NDr6qSdHlwc3rwUS1HAejWSK/RbU/ZPinpgqTDVXVska/ZZ3vW9uzc/FzHMwHciJFCr6rvq+puSVsl7bB91yJfs7+qZqpqZtPGTR3PBHAjrutd96r6StIRSbubrAHQxCjvum+yfdPg8zWS7pX0ceNdADo0yrvumyX9xfaUFv5ieKmqXms7C0CXRnnX/UNJ94xhC4BG+M04IAChAwEIHQhA6EAAQgcCEDoQgNCBAIQOBCB0IAChAwEIHQhA6EAAQgcCEDoQgNCBAIQOBCB0IAChAwEIHQhA6EAAQgcCEDoQwAvXUOz4Qe05SZ93/sCL2yhpfkzP1QeOb7KN+/h+VlU/uiZak9DHyfZsVc30vaMVjm+yLZXj41t3IAChAwGWQ+j7+x7QGMc32ZbE8U38a3QAwy2HMzqAIQgdCEDoQABCBwIQOhDgX1sTwItjkEuoAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                PWD_UPDT_TS  PH_NUM_UPDT_TS  CUST_SINCE_DT      GOAL\n",
      "PWD_UPDT_TS        1.000000        0.007984      -0.009193 -0.236107\n",
      "PH_NUM_UPDT_TS     0.007984        1.000000       0.022571 -0.034617\n",
      "CUST_SINCE_DT     -0.009193        0.022571       1.000000  0.037321\n",
      "GOAL              -0.236107       -0.034617       0.037321  1.000000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is a slight negative correlation between PWD_UPDT_TS and FRAUD_NONFRAUD as well as Phone number update time.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build the Train Dataset from scene trends"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\n",
    "\n",
    "df_train = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "num_list = ['TRAN_AMT', 'CUST_AGE', 'OPEN_ACCT_CT', 'WF_dvc_age']\n",
    "for x in num_list:\n",
    "    df_train[x] = data_frame_numeric[x].copy()\n",
    "\n",
    "str_list = ['CARR_NAME', 'RGN_NAME', 'DVC_TYPE_TXT', 'AUTHC_PRIM_TYPE_CD', 'AUTHC_SCNDRY_STAT_TXT']\n",
    "for x in str_list:\n",
    "    df_train[x] = df_strings_pre_conceived[x].copy()\n",
    "\n",
    "\n",
    "df_train['MATCHES'] = df_state_match['matches']\n",
    "\n",
    "#Add in the columns that I want\n",
    "df_train['PWD_UPDT_TS'] = df_diffs['PWD_UPDT_TS'].copy()\n",
    "df_train['PH_NUM_UPDT_TS'] = df_diffs['PH_NUM_UPDT_TS'].copy()\n",
    "\n",
    "df_train['GOAL'] = df['FRAUD_NONFRAUD'].astype('category').cat.codes  \n",
    "\n",
    "print(df_train)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       TRAN_AMT  CUST_AGE  OPEN_ACCT_CT  WF_dvc_age  CARR_NAME  RGN_NAME  \\\n",
      "0          5.38        47             4        2777        117        16   \n",
      "1         65.19        45             5        2721         72        16   \n",
      "2         54.84        36             8        1531        506         7   \n",
      "3          0.01        62             3         835        447        16   \n",
      "4        497.08        81             2        1095         94        13   \n",
      "...         ...       ...           ...         ...        ...       ...   \n",
      "13995   1937.21        55             4         142         68        16   \n",
      "13996    114.38        44            10         272        447        16   \n",
      "13997    493.00        54             3         517         32        16   \n",
      "13998    491.64        21             3           0         -1        -1   \n",
      "13999      6.02        60             6         944         74        13   \n",
      "\n",
      "       DVC_TYPE_TXT  AUTHC_PRIM_TYPE_CD  AUTHC_SCNDRY_STAT_TXT  MATCHES  \\\n",
      "0                -1                   4                      0        1   \n",
      "1                -1                   2                      0        1   \n",
      "2                 0                   4                      0        0   \n",
      "3                 1                   4                      0        0   \n",
      "4                 1                   4                      2        0   \n",
      "...             ...                 ...                    ...      ...   \n",
      "13995             1                   4                      0        1   \n",
      "13996             1                   2                      0        0   \n",
      "13997             0                   4                      0        0   \n",
      "13998             0                   4                      0        0   \n",
      "13999             1                   4                      0        0   \n",
      "\n",
      "       PWD_UPDT_TS  PH_NUM_UPDT_TS  GOAL  \n",
      "0           1203.0            68.0     1  \n",
      "1             -1.0            -1.0     1  \n",
      "2           -259.0           704.0     0  \n",
      "3            549.0           906.0     1  \n",
      "4            180.0           415.0     0  \n",
      "...            ...             ...   ...  \n",
      "13995         -1.0          1336.0     1  \n",
      "13996       1317.0            -1.0     1  \n",
      "13997        -19.0            -9.0     0  \n",
      "13998        435.0            -1.0     0  \n",
      "13999         -1.0            -1.0     1  \n",
      "\n",
      "[14000 rows x 13 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build the Test Set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "load_test = pd.read_csv('testset_for_participants.csv')\n",
    "test_df = pd.DataFrame()\n",
    "print(load_test.columns)\n",
    "#load_test['FRAUD_NONFRAUD'] = load_test['FRAUD_NONFRAUD'].astype('category').cat.codes    \n",
    "#Numerical Values\n",
    "num_list = ['TRAN_AMT', 'CUST_AGE', 'OPEN_ACCT_CT', 'WF_dvc_age']\n",
    "for x in num_list:\n",
    "    test_df[x] = load_test[x].copy()\n",
    "\n",
    "\n",
    "#String\n",
    "str_list = ['CARR_NAME', 'RGN_NAME', 'DVC_TYPE_TXT', 'AUTHC_PRIM_TYPE_CD', 'AUTHC_SCNDRY_STAT_TXT']\n",
    "str_df = pd.DataFrame()\n",
    "for x in str_list:\n",
    "    str_df[x] = load_test[x].copy()\n",
    "    str_df[x] = str_df[x].astype('category').cat.codes    \n",
    "\n",
    "for x in str_df.columns:\n",
    "    test_df[x] = str_df[x]\n",
    "\n",
    "#State Match\n",
    "\n",
    "df_state_match_test = load_test[['STATE_PRVNC_TXT',  'CUST_STATE']]\n",
    "states = {\"AL\":\"Alabama\",\"AK\":\"Alaska\",\"AZ\":\"Arizona\",\"AR\":\"Arkansas\",\"CA\":\"California\",\"CO\":\"Colorado\",\"CT\":\"Connecticut\",\"DE\":\"Delaware\",\"FL\":\"Florida\",\"GA\":\"Georgia\",\"HI\":\"Hawaii\",\"ID\":\"Idaho\",\"IL\":\"Illinois\",\"IN\":\"Indiana\",\"IA\":\"Iowa\",\"KS\":\"Kansas\",\"KY\":\"Kentucky\",\"LA\":\"Louisiana\",\"ME\":\"Maine\",\"MD\":\"Maryland\",\"MA\":\"Massachusetts\",\"MI\":\"Michigan\",\"MN\":\"Minnesota\",\"MS\":\"Mississippi\",\"MO\":\"Missouri\",\"MT\":\"Montana\",\"NE\":\"Nebraska\",\"NV\":\"Nevada\",\"NH\":\"New Hampshire\",\"NJ\":\"New Jersey\",\"NM\":\"New Mexico\",\"NY\":\"New York\",\"NC\":\"North Carolina\",\"ND\":\"North Dakota\",\"OH\":\"Ohio\",\"OK\":\"Oklahoma\",\"OR\":\"Oregon\",\"PA\":\"Pennsylvania\",\"RI\":\"Rhode Island\",\"SC\":\"South Carolina\",\"SD\":\"South Dakota\",\"TN\":\"Tennessee\",\"TX\":\"Texas\",\"UT\":\"Utah\",\"VT\":\"Vermont\",\"VA\":\"Virginia\",\"WA\":\"Washington\",\"WV\":\"West Virginia\",\"WI\":\"Wisconsin\",\"WY\":\"Wyoming\"}\n",
    "#df_state_match['match'] = (df['STATE_PRVNC_TXT'] == states[df['CUST_STATE']])\n",
    "state_match_test = df_state_match_test.to_numpy()\n",
    "\n",
    "matches = []\n",
    "for row in state_match_test:\n",
    "    try:\n",
    "        cust_state = row[1]\n",
    "        trans_loc = row[0].lower()\n",
    "        cust_state = states[cust_state.upper()].lower()\n",
    "        matches.append(int(trans_loc == cust_state))\n",
    "    except:\n",
    "        matches.append(False)\n",
    "\n",
    "test_df['matches'] = matches\n",
    "\n",
    "test_df['matches'] = test_df['matches'].astype('category').cat.codes\n",
    "\n",
    "print(test_df)\n",
    "\n",
    "#Date Time\n",
    "\n",
    "df_date_times = load_test[['PWD_UPDT_TS','TRAN_TS', 'PH_NUM_UPDT_TS', 'CUST_SINCE_DT']].copy()\n",
    "for x in df_date_times.columns:\n",
    "    \n",
    "    df_date_times[x] = pd.to_datetime(df_date_times[x], errors='coerce')\n",
    "\n",
    "df_test_diffs = pd.DataFrame()\n",
    "for x in df_date_times.columns:\n",
    "    if x != \"TRAN_TS\":\n",
    "        df_test_diffs[x] = df_date_times['TRAN_TS'] - df_date_times[x]\n",
    "        df_test_diffs[x] = df_test_diffs[x].astype('timedelta64[D]')\n",
    "        df_test_diffs[x] = df_test_diffs[x].fillna(value = -1)\n",
    "\n",
    "#df_test_diffs['GOAL'] = df['GOAL']\n",
    "\n",
    "\n",
    "test_df['PWD_UPDT_TS'] = df_test_diffs['PWD_UPDT_TS'].copy()\n",
    "test_df['PH_NUM_UPDT_TS'] = df_test_diffs['PH_NUM_UPDT_TS'].copy()\n",
    "\n",
    "print(test_df)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['dataset_id', 'TRAN_AMT', 'ACCT_PRE_TRAN_AVAIL_BAL', 'CUST_AGE',\n",
      "       'OPEN_ACCT_CT', 'WF_dvc_age', 'PWD_UPDT_TS', 'CARR_NAME', 'RGN_NAME',\n",
      "       'STATE_PRVNC_TXT', 'ALERT_TRGR_CD', 'DVC_TYPE_TXT',\n",
      "       'AUTHC_PRIM_TYPE_CD', 'AUTHC_SCNDRY_STAT_TXT', 'CUST_ZIP', 'CUST_STATE',\n",
      "       'PH_NUM_UPDT_TS', 'CUST_SINCE_DT', 'TRAN_TS', 'TRAN_DT', 'ACTN_CD',\n",
      "       'ACTN_INTNL_TXT', 'TRAN_TYPE_CD', 'ACTVY_DT'],\n",
      "      dtype='object')\n",
      "      TRAN_AMT  CUST_AGE  OPEN_ACCT_CT  WF_dvc_age  CARR_NAME  RGN_NAME  \\\n",
      "0        54.25        23             4         634         59        15   \n",
      "1       110.30        56             3        1876         70         6   \n",
      "2       515.95        33             5          40        240         4   \n",
      "3        42.78        60             3         367        305        15   \n",
      "4         0.01        61            12         518         71        15   \n",
      "...        ...       ...           ...         ...        ...       ...   \n",
      "5995    449.45        54             6         504         59        15   \n",
      "5996    488.72        64             7           0         -1        -1   \n",
      "5997      0.01        50             5         457        120        15   \n",
      "5998    211.10        37            20           6        129        15   \n",
      "5999      7.32        35            17         476         71        10   \n",
      "\n",
      "      DVC_TYPE_TXT  AUTHC_PRIM_TYPE_CD  AUTHC_SCNDRY_STAT_TXT  matches  \n",
      "0                0                   4                      0        0  \n",
      "1                1                   4                      0        0  \n",
      "2                1                   4                      0        0  \n",
      "3                1                   4                      0        1  \n",
      "4                1                   4                      0        1  \n",
      "...            ...                 ...                    ...      ...  \n",
      "5995             0                   4                      0        0  \n",
      "5996             1                   4                      0        0  \n",
      "5997             1                   4                      0        0  \n",
      "5998             3                   4                      0        1  \n",
      "5999             0                   2                      0        0  \n",
      "\n",
      "[6000 rows x 10 columns]\n",
      "      TRAN_AMT  CUST_AGE  OPEN_ACCT_CT  WF_dvc_age  CARR_NAME  RGN_NAME  \\\n",
      "0        54.25        23             4         634         59        15   \n",
      "1       110.30        56             3        1876         70         6   \n",
      "2       515.95        33             5          40        240         4   \n",
      "3        42.78        60             3         367        305        15   \n",
      "4         0.01        61            12         518         71        15   \n",
      "...        ...       ...           ...         ...        ...       ...   \n",
      "5995    449.45        54             6         504         59        15   \n",
      "5996    488.72        64             7           0         -1        -1   \n",
      "5997      0.01        50             5         457        120        15   \n",
      "5998    211.10        37            20           6        129        15   \n",
      "5999      7.32        35            17         476         71        10   \n",
      "\n",
      "      DVC_TYPE_TXT  AUTHC_PRIM_TYPE_CD  AUTHC_SCNDRY_STAT_TXT  matches  \\\n",
      "0                0                   4                      0        0   \n",
      "1                1                   4                      0        0   \n",
      "2                1                   4                      0        0   \n",
      "3                1                   4                      0        1   \n",
      "4                1                   4                      0        1   \n",
      "...            ...                 ...                    ...      ...   \n",
      "5995             0                   4                      0        0   \n",
      "5996             1                   4                      0        0   \n",
      "5997             1                   4                      0        0   \n",
      "5998             3                   4                      0        1   \n",
      "5999             0                   2                      0        0   \n",
      "\n",
      "      PWD_UPDT_TS  PH_NUM_UPDT_TS  \n",
      "0           112.0           915.0  \n",
      "1           -79.0        100000.0  \n",
      "2        100000.0           968.0  \n",
      "3           119.0           715.0  \n",
      "4          1175.0           303.0  \n",
      "...           ...             ...  \n",
      "5995       1394.0           189.0  \n",
      "5996        373.0           -95.0  \n",
      "5997     100000.0           225.0  \n",
      "5998       -257.0        100000.0  \n",
      "5999        876.0           335.0  \n",
      "\n",
      "[6000 rows x 12 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Various Models (SKLearn)\n",
    "The goal here is to get a sense of what does and doesn't work with this dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Try a BASIC Random Forrest\n",
    "    Random Forest : 0.9672333086967233"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "rf_train, rf_val = train_test_split(df_train, test_size=0.25)\n",
    "#train_set, val_set = train_test_split(rf_train, test_size = .2)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators = 200,max_features ='sqrt', min_samples_leaf= 4)\n",
    "\n",
    "rf_model = rf_model.fit(rf_train.loc[:, rf_train.columns != 'GOAL'], rf_train['GOAL'])\n",
    "val_pred = rf_model.predict(rf_val.loc[:, rf_val.columns != 'GOAL'])\n",
    "\n",
    "\n",
    "print(f1_score(val_pred, rf_val['GOAL']))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9585070478459401\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear Regression and Logistic Regression\n",
    "    Linear Regression : 0.8629897679061641\n",
    "    Logistic Regression : 0.861796643632774"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_t, val_t = rf_train, rf_val#train_test_split(df_train, test_size=0.2)\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "lr_model = lr_model.fit(train_t.loc[:, train_t.columns != 'GOAL'], train_t['GOAL'])\n",
    "val_pred = lr_model.predict(val_t.loc[:, val_t.columns != 'GOAL'])\n",
    "val_pred = (val_pred > .5)\n",
    "print(\"Linear Regression : \" + str(f1_score(val_pred, val_t['GOAL'])))\n",
    "\n",
    "\n",
    "lg_model = LogisticRegression(solver ='liblinear')\n",
    "\n",
    "lg_model = lg_model.fit(train_t.loc[:, train_t.columns != 'GOAL'], train_t['GOAL'])\n",
    "val_pred = lg_model.predict(val_t.loc[:, val_t.columns != 'GOAL'])\n",
    "val_pred = (val_pred > .5)\n",
    "print(\"Linear Regression : \" + str(f1_score(val_pred, val_t['GOAL'])))\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear Regression : 0.885188770571152\n",
      "Linear Regression : 0.8897485493230174\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SKLearn's MLP model\n",
    "    MLP Classifier : 0.8819609709662064"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "random.seed(1)\n",
    "train_mlp, val_mlp = rf_train, rf_val#train_test_split(df_train, test_size=0.2)\n",
    "\n",
    "mlp = MLPClassifier((5, 100), random_state = 1, activation='relu', solver = 'adam', alpha = .0001, learning_rate_init= .001, learning_rate = 'constant', max_iter = 1000)\n",
    "\n",
    "mlp = mlp.fit(train_mlp.loc[:, train_mlp.columns != 'GOAL'], train_mlp['GOAL'])\n",
    "val_pred = mlp.predict(val_mlp.loc[:, val_mlp.columns != 'GOAL'])\n",
    "print(f1_score(val_pred, val_mlp['GOAL']))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9049049049049049\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train A Deep Learning Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import torch.nn as nn\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class input_dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data_x = data.loc[:, data.columns != 'GOAL'].to_numpy()\n",
    "        self.data_y = data.loc[:, data.columns == 'GOAL'].to_numpy()\n",
    "        temp = []\n",
    "        for x in self.data_y :\n",
    "            if x == 1:\n",
    "                temp.append([0,1])\n",
    "            else:\n",
    "                temp.append([1,0])\n",
    "        self.data_y = np.array(temp)\n",
    "    def __getitem__(self, i):\n",
    "        return (self.data_x[i, :], self.data_y[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "BATCH_SIZE = 8\n",
    "#print(df_train.loc[0,:])\n",
    "#df_train.loc[:,] = (df_train- df_train.mean())/df_train.std()\n",
    "\n",
    "train_ds, val = train_test_split(df_train, test_size=0.2)\n",
    "\n",
    "train_dataset = input_dataset(train_ds)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle= True)\n",
    "\n",
    "val_dataset = input_dataset(val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle= True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the MLP framework"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#This gives the opportunity to do a single predictive output node or two and use the max of the two as probability\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        #self.fc3 = nn.Linear(64, 256)\n",
    "        self.dropout = nn.Dropout(p=.2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.out = nn.Linear(64, output_dim)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.out(x)\n",
    "        if self.output_dim != 1:\n",
    "            x = self.softmax(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train and Eval methods"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for (X, Y) in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(X.float())\n",
    "        predValue, preds = torch.max(predictions, 1)\n",
    "        #print(predictions)\n",
    "        #print(Y)\n",
    "        _, t_val = torch.max(Y.float(), 1)\n",
    "        loss = criterion(predictions.float(), Y.float())\n",
    "        \n",
    "        acc = f1_score(t_val, preds)\n",
    "        \n",
    "        #print(acc)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for (X,Y) in iterator:\n",
    "            #print(X)\n",
    "            predictions = model(X.float())\n",
    "            predValue, preds = torch.max(predictions, 1)\n",
    "            \n",
    "            loss = criterion(predictions.float(), Y.float())\n",
    "            \n",
    "            _, t_val = torch.max(Y.float(), 1)\n",
    "            acc = f1_score(t_val, preds)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "exp_model = MLP(len(df_train.columns) -1,2)\n",
    "\n",
    "\n",
    "\n",
    "exp_optimizer = optim.Adam(exp_model.parameters(), lr = .0001, weight_decay = .01)\n",
    "exp_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "N_EPOCHS = 100\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(exp_model, train_dataloader, exp_optimizer, exp_criterion)\n",
    "    valid_loss, valid_acc = evaluate(exp_model, val_dataloader, exp_criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(exp_model.state_dict(), 'tut3-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train F1 Score: {train_acc:.2f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val F1 Score: {valid_acc:.2f}')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.652 | Train F1 Score: 0.81\n",
      "\t Val. Loss: 0.621 |  Val F1 Score: 0.84\n",
      "Epoch: 02 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.627 | Train F1 Score: 0.82\n",
      "\t Val. Loss: 0.616 |  Val F1 Score: 0.84\n",
      "Epoch: 03 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.623 | Train F1 Score: 0.83\n",
      "\t Val. Loss: 0.614 |  Val F1 Score: 0.84\n",
      "Epoch: 04 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.620 | Train F1 Score: 0.83\n",
      "\t Val. Loss: 0.611 |  Val F1 Score: 0.84\n",
      "Epoch: 05 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.616 | Train F1 Score: 0.84\n",
      "\t Val. Loss: 0.608 |  Val F1 Score: 0.85\n",
      "Epoch: 06 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.613 | Train F1 Score: 0.83\n",
      "\t Val. Loss: 0.606 |  Val F1 Score: 0.85\n",
      "Epoch: 07 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.610 | Train F1 Score: 0.84\n",
      "\t Val. Loss: 0.603 |  Val F1 Score: 0.86\n",
      "Epoch: 08 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.608 | Train F1 Score: 0.85\n",
      "\t Val. Loss: 0.601 |  Val F1 Score: 0.87\n",
      "Epoch: 09 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.607 | Train F1 Score: 0.85\n",
      "\t Val. Loss: 0.600 |  Val F1 Score: 0.87\n",
      "Epoch: 10 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.605 | Train F1 Score: 0.85\n",
      "\t Val. Loss: 0.599 |  Val F1 Score: 0.87\n",
      "Epoch: 11 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.605 | Train F1 Score: 0.86\n",
      "\t Val. Loss: 0.599 |  Val F1 Score: 0.86\n",
      "Epoch: 12 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.604 | Train F1 Score: 0.85\n",
      "\t Val. Loss: 0.597 |  Val F1 Score: 0.87\n",
      "Epoch: 13 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.602 | Train F1 Score: 0.86\n",
      "\t Val. Loss: 0.596 |  Val F1 Score: 0.87\n",
      "Epoch: 14 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.600 | Train F1 Score: 0.86\n",
      "\t Val. Loss: 0.592 |  Val F1 Score: 0.88\n",
      "Epoch: 15 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.597 | Train F1 Score: 0.87\n",
      "\t Val. Loss: 0.590 |  Val F1 Score: 0.88\n",
      "Epoch: 16 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.596 | Train F1 Score: 0.87\n",
      "\t Val. Loss: 0.589 |  Val F1 Score: 0.89\n",
      "Epoch: 17 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.594 | Train F1 Score: 0.87\n",
      "\t Val. Loss: 0.589 |  Val F1 Score: 0.89\n",
      "Epoch: 18 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.595 | Train F1 Score: 0.87\n",
      "\t Val. Loss: 0.589 |  Val F1 Score: 0.88\n",
      "Epoch: 19 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.594 | Train F1 Score: 0.87\n",
      "\t Val. Loss: 0.587 |  Val F1 Score: 0.89\n",
      "Epoch: 20 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.593 | Train F1 Score: 0.87\n",
      "\t Val. Loss: 0.587 |  Val F1 Score: 0.89\n",
      "Epoch: 21 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.593 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.586 |  Val F1 Score: 0.89\n",
      "Epoch: 22 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.591 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.586 |  Val F1 Score: 0.89\n",
      "Epoch: 23 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.591 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.586 |  Val F1 Score: 0.89\n",
      "Epoch: 24 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.592 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.586 |  Val F1 Score: 0.89\n",
      "Epoch: 25 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.593 | Train F1 Score: 0.87\n",
      "\t Val. Loss: 0.586 |  Val F1 Score: 0.89\n",
      "Epoch: 26 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.593 | Train F1 Score: 0.87\n",
      "\t Val. Loss: 0.585 |  Val F1 Score: 0.89\n",
      "Epoch: 27 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.590 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.584 |  Val F1 Score: 0.89\n",
      "Epoch: 28 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.590 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.584 |  Val F1 Score: 0.89\n",
      "Epoch: 29 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.589 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.583 |  Val F1 Score: 0.89\n",
      "Epoch: 30 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.590 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.583 |  Val F1 Score: 0.89\n",
      "Epoch: 31 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.588 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.582 |  Val F1 Score: 0.90\n",
      "Epoch: 32 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.589 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.582 |  Val F1 Score: 0.89\n",
      "Epoch: 33 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.588 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.582 |  Val F1 Score: 0.90\n",
      "Epoch: 34 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.589 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.582 |  Val F1 Score: 0.90\n",
      "Epoch: 35 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.587 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.89\n",
      "Epoch: 36 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.588 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.583 |  Val F1 Score: 0.89\n",
      "Epoch: 37 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.587 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.582 |  Val F1 Score: 0.89\n",
      "Epoch: 38 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.588 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.89\n",
      "Epoch: 39 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.587 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.582 |  Val F1 Score: 0.89\n",
      "Epoch: 40 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.587 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.90\n",
      "Epoch: 41 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.588 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.582 |  Val F1 Score: 0.89\n",
      "Epoch: 42 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.587 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.89\n",
      "Epoch: 43 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.587 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.582 |  Val F1 Score: 0.90\n",
      "Epoch: 44 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.587 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.90\n",
      "Epoch: 45 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.586 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.582 |  Val F1 Score: 0.89\n",
      "Epoch: 46 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.587 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.90\n",
      "Epoch: 47 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.586 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.90\n",
      "Epoch: 48 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.587 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.89\n",
      "Epoch: 49 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.587 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.582 |  Val F1 Score: 0.89\n",
      "Epoch: 50 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.586 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.582 |  Val F1 Score: 0.89\n",
      "Epoch: 51 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.587 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.582 |  Val F1 Score: 0.89\n",
      "Epoch: 52 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.587 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.90\n",
      "Epoch: 53 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.587 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.90\n",
      "Epoch: 54 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.585 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.89\n",
      "Epoch: 55 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.585 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.580 |  Val F1 Score: 0.90\n",
      "Epoch: 56 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.586 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.583 |  Val F1 Score: 0.89\n",
      "Epoch: 57 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.586 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.90\n",
      "Epoch: 58 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.586 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.89\n",
      "Epoch: 59 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.586 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.580 |  Val F1 Score: 0.89\n",
      "Epoch: 60 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.586 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.580 |  Val F1 Score: 0.90\n",
      "Epoch: 61 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.586 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.89\n",
      "Epoch: 62 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.587 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.580 |  Val F1 Score: 0.90\n",
      "Epoch: 63 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.586 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.89\n",
      "Epoch: 64 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.585 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.580 |  Val F1 Score: 0.90\n",
      "Epoch: 65 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.585 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.580 |  Val F1 Score: 0.90\n",
      "Epoch: 66 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.586 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.580 |  Val F1 Score: 0.90\n",
      "Epoch: 67 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.586 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.580 |  Val F1 Score: 0.89\n",
      "Epoch: 68 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.585 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.580 |  Val F1 Score: 0.90\n",
      "Epoch: 69 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.585 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.581 |  Val F1 Score: 0.89\n",
      "Epoch: 70 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.585 | Train F1 Score: 0.88\n",
      "\t Val. Loss: 0.579 |  Val F1 Score: 0.90\n",
      "Epoch: 71 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.583 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.577 |  Val F1 Score: 0.90\n",
      "Epoch: 72 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.583 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.577 |  Val F1 Score: 0.90\n",
      "Epoch: 73 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.583 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.578 |  Val F1 Score: 0.90\n",
      "Epoch: 74 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.582 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.577 |  Val F1 Score: 0.90\n",
      "Epoch: 75 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.582 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.576 |  Val F1 Score: 0.90\n",
      "Epoch: 76 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.581 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.577 |  Val F1 Score: 0.90\n",
      "Epoch: 77 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.582 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.576 |  Val F1 Score: 0.90\n",
      "Epoch: 78 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.583 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.578 |  Val F1 Score: 0.90\n",
      "Epoch: 79 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.582 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.577 |  Val F1 Score: 0.90\n",
      "Epoch: 80 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.582 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.576 |  Val F1 Score: 0.90\n",
      "Epoch: 81 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.583 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.577 |  Val F1 Score: 0.90\n",
      "Epoch: 82 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.582 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.578 |  Val F1 Score: 0.90\n",
      "Epoch: 83 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.581 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.577 |  Val F1 Score: 0.90\n",
      "Epoch: 84 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.583 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.576 |  Val F1 Score: 0.90\n",
      "Epoch: 85 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.582 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.576 |  Val F1 Score: 0.90\n",
      "Epoch: 86 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.582 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.577 |  Val F1 Score: 0.90\n",
      "Epoch: 87 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.581 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.576 |  Val F1 Score: 0.90\n",
      "Epoch: 88 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.582 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.578 |  Val F1 Score: 0.90\n",
      "Epoch: 89 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.582 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.576 |  Val F1 Score: 0.90\n",
      "Epoch: 90 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.581 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.578 |  Val F1 Score: 0.90\n",
      "Epoch: 91 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.581 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.576 |  Val F1 Score: 0.90\n",
      "Epoch: 92 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.582 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.577 |  Val F1 Score: 0.90\n",
      "Epoch: 93 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.581 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.576 |  Val F1 Score: 0.90\n",
      "Epoch: 94 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.582 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.576 |  Val F1 Score: 0.90\n",
      "Epoch: 95 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.581 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.575 |  Val F1 Score: 0.90\n",
      "Epoch: 96 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.582 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.576 |  Val F1 Score: 0.91\n",
      "Epoch: 97 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.581 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.578 |  Val F1 Score: 0.90\n",
      "Epoch: 98 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.582 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.576 |  Val F1 Score: 0.90\n",
      "Epoch: 99 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.581 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.577 |  Val F1 Score: 0.90\n",
      "Epoch: 100 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.582 | Train F1 Score: 0.89\n",
      "\t Val. Loss: 0.576 |  Val F1 Score: 0.90\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use the class probabilities for the random forest in a deep learning model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train a random forest for the purpose of feeding into an MLP at the bottom Layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "rf_data, mlp_train = train_test_split(df_train, test_size=0.5)\n",
    "\n",
    "rf_train_mlp, rf_val_mlp = train_test_split(rf_data, test_size = .2)\n",
    "\n",
    "rf_model_mlp_feeder = RandomForestClassifier(n_estimators = 200,max_features ='sqrt', min_samples_leaf= 4)\n",
    "\n",
    "rf_model_mlp_feeder = rf_model_mlp_feeder.fit(rf_train_mlp.loc[:, rf_train_mlp.columns != 'GOAL'], rf_train_mlp['GOAL'])\n",
    "rf_mlp_pred = rf_model_mlp_feeder.predict(rf_val_mlp.loc[:, rf_val_mlp.columns != 'GOAL'])\n",
    "\n",
    "\n",
    "print(f1_score(rf_mlp_pred, rf_val_mlp['GOAL']))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9538934426229508\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import pickle\n",
    "pickle.dump(rf_model_mlp_feeder, open('mlp_feeder', 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Definition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "#This gives the opportunity to do a single predictive output node or two and use the max of the two as probability\n",
    "class MLP_with_RF_preds(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        #self.fc3 = nn.Linear(64, 256)\n",
    "        self.dropout = nn.Dropout(p=.2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.out = nn.Linear(64 + 2, output_dim)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "    def forward(self, x, rf):\n",
    "        x = self.fc1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.tanh(x)\n",
    "        x = torch.cat((x, rf),1 )\n",
    "        x = self.out(x)\n",
    "        if self.output_dim != 1:\n",
    "            x = self.softmax(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train and Eval Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def train_with_rf(model, iterator, optimizer, criterion, output_dim):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for (X, R, Y) in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(X.float(), R.float())\n",
    "        if output_dim > 1:\n",
    "            predValue, preds = torch.max(predictions, 1)\n",
    "        #print(predictions)\n",
    "        #print(Y)\n",
    "            _, t_val = torch.max(Y.float(), 1)\n",
    "            loss = criterion(predictions.float(), Y.float())\n",
    "        \n",
    "            acc = f1_score(t_val, preds)\n",
    "        else:\n",
    "           \n",
    "            loss = criterion(predictions.float(), Y.float())\n",
    "            for v in temp:\n",
    "                if v < .5:\n",
    "                    temp.append(0)\n",
    "                else:\n",
    "                    temp.append(1)\n",
    "            predictions = temp\n",
    "            acc = f1_score(predictions, Y.float())\n",
    "        #print(acc)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def evaluate_with_rf(model, iterator, criterion, output_dim):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for (X,R, Y) in iterator:\n",
    "            #print(X)\n",
    "            predictions = model(X.float(), R.float())\n",
    "            if output_dim > 1:\n",
    "                predValue, preds = torch.max(predictions, 1)\n",
    "                \n",
    "                loss = criterion(predictions.float(), Y.float())\n",
    "                \n",
    "                _, t_val = torch.max(Y.float(), 1)\n",
    "                acc = f1_score(t_val, preds)\n",
    "            else:\n",
    "                loss = criterion(predictions.float(), Y.float())\n",
    "                temp = []\n",
    "                for v in temp:\n",
    "                    if v < .5:\n",
    "                        temp.append(0)\n",
    "                    else:\n",
    "                        temp.append(1)\n",
    "                predictions = temp\n",
    "                acc = f1_score(predictions, Y.float())\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The two classes are for two different styles. I can represent this as a regression task and classify by .5 or I can I represent it as two classes and take the max"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "class input_dataset_with_rf_2(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data_x = data.loc[:, 'TRAN_AMT' : 'PH_NUM_UPDT_TS'].to_numpy()\n",
    "        self.rf_data = data.loc[:, 'Class_RF_Pred' : 'Class_Prob'].to_numpy()\n",
    "        self.data_y = data.loc[:, data.columns == 'GOAL'].to_numpy()\n",
    "        temp = []\n",
    "        for x in self.data_y :\n",
    "            if x == 1:\n",
    "                temp.append([0,1])\n",
    "            else:\n",
    "                temp.append([1,0])\n",
    "        self.data_y = np.array(temp)\n",
    "    def __getitem__(self, i):\n",
    "        return (self.data_x[i, :], self.rf_data[i, :], self.data_y[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "class input_dataset_with_rf_1(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data_x = data.loc[:, 'TRAN_AMT' : 'PH_NUM_UPDT_TS'].to_numpy()\n",
    "        self.rf_data = data.loc[:, 'Class_RF_Pred' : 'Class_Prob'].to_numpy()\n",
    "        self.data_y = data.loc[:, data.columns == 'GOAL'].to_numpy()\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return (self.data_x[i, :], self.rf_data[i, :], self.data_y[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "\n",
    "predict_class = rf_model.predict_proba(mlp_train.loc[:, val.columns != 'GOAL'])\n",
    "\n",
    "new_df = mlp_train.copy()\n",
    "temp = []\n",
    "temp_val = []\n",
    "for row in predict_class:\n",
    "    max_val = max(row)\n",
    "    temp_val.append(max_val)\n",
    "    temp.append(list(row).index(max_val))\n",
    "new_df['Class_RF_Pred'] = temp\n",
    "new_df['Class_Prob'] = temp_val\n",
    "print(new_df.columns)\n",
    "rf_df, rf_val = train_test_split(new_df, test_size=0.05)\n",
    "\n",
    "rf_train_dataset = input_dataset_with_rf_2(rf_df)\n",
    "rf_train_dataloader = DataLoader(rf_train_dataset, batch_size = BATCH_SIZE, shuffle= True)\n",
    "\n",
    "rf_val_dataset = input_dataset_with_rf_2(rf_val)\n",
    "rf_val_dataloader = DataLoader(rf_val_dataset, batch_size = BATCH_SIZE, shuffle= True)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['TRAN_AMT', 'CUST_AGE', 'OPEN_ACCT_CT', 'WF_dvc_age', 'CARR_NAME',\n",
      "       'RGN_NAME', 'DVC_TYPE_TXT', 'AUTHC_PRIM_TYPE_CD',\n",
      "       'AUTHC_SCNDRY_STAT_TXT', 'MATCHES', 'PWD_UPDT_TS', 'PH_NUM_UPDT_TS',\n",
      "       'GOAL', 'Class_RF_Pred', 'Class_Prob'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "\n",
    "new_model = MLP_with_RF_preds(len(new_df.columns) -3,2)\n",
    "\n",
    "new_optimizer = optim.Adam(new_model.parameters(), lr = .005)\n",
    "new_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "N_EPOCHS = 15\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_with_rf(new_model, rf_train_dataloader, new_optimizer, new_criterion,2)\n",
    "    valid_loss, valid_acc = evaluate_with_rf(new_model, rf_val_dataloader, new_criterion, 2)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(new_model.state_dict(), 'best-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train F1 Score: {train_acc:.5f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val F1 Score: {valid_acc:.5f}')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.580 | Train F1 Score: 0.90563\n",
      "\t Val. Loss: 0.536 |  Val F1 Score: 0.97520\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.532 | Train F1 Score: 0.96818\n",
      "\t Val. Loss: 0.528 |  Val F1 Score: 0.97062\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.526 | Train F1 Score: 0.96933\n",
      "\t Val. Loss: 0.526 |  Val F1 Score: 0.97235\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.524 | Train F1 Score: 0.97118\n",
      "\t Val. Loss: 0.524 |  Val F1 Score: 0.97333\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.523 | Train F1 Score: 0.96902\n",
      "\t Val. Loss: 0.524 |  Val F1 Score: 0.97523\n",
      "Epoch: 06 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.523 | Train F1 Score: 0.96965\n",
      "\t Val. Loss: 0.524 |  Val F1 Score: 0.97240\n",
      "Epoch: 07 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.523 | Train F1 Score: 0.96978\n",
      "\t Val. Loss: 0.523 |  Val F1 Score: 0.97152\n",
      "Epoch: 08 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.523 | Train F1 Score: 0.96985\n",
      "\t Val. Loss: 0.524 |  Val F1 Score: 0.97161\n",
      "Epoch: 09 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.523 | Train F1 Score: 0.96969\n",
      "\t Val. Loss: 0.523 |  Val F1 Score: 0.97072\n",
      "Epoch: 10 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.523 | Train F1 Score: 0.97031\n",
      "\t Val. Loss: 0.523 |  Val F1 Score: 0.97369\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 11 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.523 | Train F1 Score: 0.96791\n",
      "\t Val. Loss: 0.523 |  Val F1 Score: 0.97119\n",
      "Epoch: 12 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.523 | Train F1 Score: 0.96865\n",
      "\t Val. Loss: 0.523 |  Val F1 Score: 0.97285\n",
      "Epoch: 13 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.523 | Train F1 Score: 0.96804\n",
      "\t Val. Loss: 0.524 |  Val F1 Score: 0.97231\n",
      "Epoch: 14 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.523 | Train F1 Score: 0.96972\n",
      "\t Val. Loss: 0.523 |  Val F1 Score: 0.97164\n",
      "Epoch: 15 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.523 | Train F1 Score: 0.96945\n",
      "\t Val. Loss: 0.523 |  Val F1 Score: 0.97401\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The random forest model showed the most promise out of all of the models that I experimented with. Where most experienced an F1 of .85-.88 it had around .95-.96"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I was further able to push the limit of this score by implementing the output of the random forest into a neural network, specifically an MLP.\n",
    "\n",
    "I tried doing this in two ways\n",
    "\n",
    "1.) I tried adding the output of the Random Forest to the input layer, \n",
    "\n",
    " The Problem: The model never actually learned just how relevant these new features were compared to other features\n",
    "\n",
    "2.) I injected the new features into the output layer of the neural network\n",
    "\n",
    "In other words, I took the outputs of the RF and joined it with the output from the hidden layers. This was then run through a linear layer and the model was able to associate the importance of these particular features to boost F1 to about .975"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}